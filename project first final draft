---
title: "Project 1"
author: "Group"
date: "Spring 2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
 
```{r}
 
options(digits = 2)
 
suppressMessages(library(POE5Rdata))
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))
suppressMessages(library(kableExtra))
suppressMessages(library(broom))
suppressMessages(library(psych))
 
#1
data("vegas5")
stat_sum <- describe(vegas5)[,-c(1,6,7,10)]
kable(stat_sum,
      caption = "Statistical Summary")
attach(vegas5)
 
 
```
 
 
# Question 1
 
 
*vegas5* dataset with variables *default*, *arm*, *refinance*, *lien2*, *term30*, *underwater*, *ltv*, *rate*, *amount* and *fico*.
 
 
Observations: 10,000 mortgage loan observations from Las Vegas, Nevada, single family homes, 2010
 
 
default         = 1 if payment late by 90+ days,
arm             = 1 if adjustable rate mortgage, 0 if fixed,
refinance       = 1 if loan is for a refinance of any type (0 if for purchase),
lien2           = 1 if 2nd lien mortage (0 if 1st lien),
term30          = 1 if 30 year mortgage (0 if 15 year mortgage),
underwater      = 1 if borrower estimated to owe more than property worth at time of observing (0 otherwise),
ltv             loan to value ratio of property at origination (percent),
rate            current interest rate on loan (percent),
amount          loan amount in $10,000 units,
fico            borrower's credit score at origination.
 
 
 
Based on the statistical summary table, 37% of households' payments were late by 90+ days,
38% of households had adjustable rate mortgages, almost more than half of households had loans for refinance of any type. In addition, 10% of households had 2nd lien mortgages, 85% of households had 30-yr mortgages, 82% of households owed morethan the property worth at time of observing. Mean loan-to-value ratio is 69% which is pretty good, with median 78% and max 109%. Mean rate on loans is 5.98%, with min, median and max 0.5%, 6.2% and 17%, respectively. A mean amount of borrowing is $245,600, with min of $11,000, median $211,000, max $7,500,000. Regarding FICO scores, mean score is 685, with min, median and max being 442, 687, 823, respectively. Variables that deal with dollar values are not normally distributed based on skewness and kurtosis stats.
 
 
Histograms
 
```{r setup,warning=FALSE, message=FALSE}
 
 
a5 <- ggplot(vegas5) +
  geom_histogram(aes(x = log(amount), fill = factor(lien2)), colour = "black")+
            theme_bw() + ylab("frequency") + labs(fill = "lien2")
 
a6 <- ggplot(vegas5) +
  geom_histogram(aes(x = log(amount), fill = factor(default)),
                 colour = "black") + labs(fill = "default") +
  theme_bw() + ylab("frequency")
 
a7 <- ggplot(vegas5) +
  geom_histogram(aes(x = log(amount), fill = factor(arm)),
                 colour = "black") + labs(fill = "arm") + theme_bw() +
  ylab("frequency")
 
a8 <- ggplot(vegas5) +
  geom_histogram(aes(x = log(amount), fill = factor(underwater)),
                 colour = "black") + labs(fill = "underwater") + theme_bw() +
  ylab("frequency")
 
grid.arrange(a5,a6,a7,a8, top = "Log(amount) Variation by Categorial Variable")
```
 
The lien graph shows that there are far less number of 2nd lien mortgages than 1st lien mortgages. And the loan amount of those 2nd lien mortgages are also smaller than the amount of the 1st lien mortgages. From the other 3 graphs we can see that: 1) there are less number of defaults than those that are paid on time; 2) there are less adjustable rate mortgages than fixed rate mortgages; 3) there are slightly less underwater mortgages than those that are not; 4) interestingly, the two cases for the 3 variables, default, arm, underwater, exhibits the similar variation.
 
 
```{r, results='hide'}
 
fd  <-  function(x) {
  n=length(x)
  r=IQR(x)
  2*r/n^(1/3)
}
 
a1 <- vegas5 %>% filter(amount < median(amount)) %>% ggplot(aes(x = amount)) +
  geom_histogram(colour = "black", fill = "steelblue") +
  aes(y = stat(count)/sum(stat(count))) +
  scale_y_continuous(labels=scales::percent) +
  ylab("Frequency (%)") +
  xlab("Amount (10K)") +
  facet_wrap(~refinance,
             labeller = labeller(refinance = c(`0`="loans for purchase",
                                               `1`="loans for refinance"))) +
  theme_bw() + ggtitle("Frequency of Loans for Single Family Homes by Amount (less than 210K)")
 
 
a2 <- vegas5 %>% filter(amount > median(amount) & amount < 180) %>% ggplot(aes(x = amount)) +
  geom_histogram( colour = "black", fill = "steelblue") +
  aes(y = stat(count)/sum(stat(count))) +
  scale_y_continuous(labels=scales::percent) + xlab("Amount (10K)") +
  ylab("Frequency(%)") + facet_wrap(~refinance,
                         labeller = labeller(refinance = c(`0`="loans for purchase",
                                                           `1`="loans for refinance"))) +
  theme_bw() + ggtitle("Frequency of Loans for Single Family
                       Homes by Amount (greater than 210K & less than 1.8Mn)")
grid.arrange(a1, a2)
 
 
 
```
 
In this plot, three variables are plotted: amount, frequency of occurrence and refinance.
We can observe a peculiar trend when plotting observations based on the filters.
Households with less than 210K borrowed seem to exhibit a left-skewness whereas households who borrowed between 210K and 1.8Mn, skewness to the right, which indicates that both distributions are not normal.
 
 
 
 
```{r}
 
suppressMessages(library(PupillometryR))
a4 <- ggplot(vegas5, aes(x = factor(default),
                   y = fico, colour = factor(arm))) + geom_jitter(alpha = 0.20) +
   geom_boxplot() + xlab("Default") +
  theme_bw() + labs(colour = "arm") +
  ylab("FICO")
 
 
a3 <- ggplot(vegas5,aes(x = factor(term30), y = fico, colour = factor(lien2))) +
  geom_jitter(alpha = 0.20) +
 geom_boxplot() + ylab("FICO") + xlab("30-yr Mortage") +
  labs(colour = "2nd Lien") +
   theme_bw()
 
grid.arrange(a4,a3, ncol = 2,
             top = "FICO Score Differences Based on Default, 30-yr Mortage, Adjustable Rate, 2nd Lien")
 
```
 
In this plot, five variables are plotted in the form of boxplots. On the left, we can see FICO scores tend to be higher for those who were not late on their payments (90+ days) and with adjustable rate mortgages (quite surprising). This may be due to the fact that interest rates might have been lower during that period. What is not surprising to observe is that FICO scores for those who were late on their payments are lower, but tend to be similar whether those with lower scores had an adjustable rates or not.
 
On the right, we can see that individuals who applied for a 15-year mortgage have a slightly higher FICO score than the ones with 30-year mortgage. This is intuitive because 15-year mortgages tend to have a higher periodic payment than 30-year, which banks would understandably require a higher FICO score. And in the same boxplot, we can see that 1st lien mortgages have a noticeably lower FICO score than the 2nd lien mortgages. This is the case probably because in the case of default, 1st lien would be paid in full before 2nd lien would be paid. Therefore 2nd lien would require a higher FICO score.
 
 
```{r, results="hide"}
 
fd(rate)
ggplot(vegas5) +
  geom_histogram(aes(x = rate), binwidth = fd, colour = "black", fill = "steelblue") +
  aes(y = ..count../sum(count)) +
  scale_y_continuous(labels = scales::percent) +
  ylab("Frequency (%)") + xlab("Rates (%)") + facet_grid(refinance~underwater, switch = "both",
                                         labeller = labeller(
                                           refinance = c(`0` = "for purchase",
                                                          `1`= "for refinance"),
                                           underwater = c(`0` = "not owe more than property worth",
                                                          `1` = "owes more than property worth")
                                         )) + theme_bw() +
  ggtitle("Rates for Households Based on Refinance Goals and Underwater Disposition")
 
```
 
As we can see, households who owe more than property worth tend to have higher mortgage rates than their non-underwater counterparts.
 
 
 
```{r}
 
ggplot(vegas5, aes(x = fico, y = ltv, colour = factor(term30))) +
  geom_point(alpha =0.20) +
  geom_smooth() +
  facet_grid(arm~underwater,
             switch = "both",
             labeller = labeller(arm = c(`0` = "fixed rate",
                             `1`= "adjustable rate"),
               underwater = c(`0` = "not owe more than property worth",
                              `1` = "owes more than property worth"))) +
  xlab("FICO") + ylab("Loan-to-Value") +
  labs(colour = "30-yr mortage") +
  ggtitle("Loan-to-Value vs FICO scores (30-year Mortage = 1, 15-yr Mortgage = 0)") +
  theme_bw()
 
```
 
Here we can observe that households who owe more than property worth have higher loan-to-value ratios,
but the relationship betwee FICO scores and ltv's is unclear where as for those who do not owe more than property worth tend to have lower ltv ratios and the relationship between FICO scores and ltv's is somewhat negative, specifically for those who did not take out a 30-yr mortgage in the first place. Although it looks somewhat positive for those who took out 30-yr mortgages. We can deduce that there is a larger concentration of households who possess 30-yr mortages and owe more than property worth. This potentially can be a byproduct of the 2007-2008 housing market crash, i.e. people owing more than their house is worth.
 
 
 
```{r}
 
#correlation plot
suppressMessages(library(corrplot))
corrplot(cor(vegas5[,7:10]))
 
 
```
 
There are some interesting correlations between variables in the correlation plot. First, we can see a slight positive correlation between FICO score and loan amount. This is explainable because banks want higher credibility for larger loans. We can also see a negative correlation between FICO score and mortgage rate. This might be explained by banks being more willing to give low rates to more creditworthy borrowers. Additionally, we can see a small positive correlation between FICO score and loan amount. Obviously, banks tend to grant larger loans to borrowers with a higher credit score.
 
 
 
```{r}
#scatterplot
suppressMessages(library(car))
scatterplotMatrix(vegas5[,7:10], col = "steelblue",
                  ellipse = TRUE)
 
```
 
Similar to descriptions for the correlation plot above, this scatterplot matrix confirms our initial observations regarding variable correlations. First, looking at the upper diagonal of the matrix, we can see that rate and loan-to-value are negatively correlated. Second, amount and loan-to-value tend to be positively correlated since the regression line is seen on the left corner inside the micro-scatter plot. Third, there might be a negative correlation between FICO and loan-to-value, it is so small that it looks almost horizontal, which leads us to believe that this correlation is negligible.
 
# Question 2
 
```{r}
#initial setup
 
t.critical <- function(alpha, df, tail = "two") {
  if(tail == "two"){
    result <- qt(1-alpha/2  , df)
  } else if(tail == "right"){
    result <- qt(1-alpha  , df)
  } else if(tail=="left"){
    result <- qt(alpha  , df)
  }
  return(result)
}
#data
library(POE5Rdata)
data("vegas5")
attach(vegas5)
#Libraries
library(POE5Rdata)
library(dynlm) #for the `dynlm()` function
library(broom) #for `glance(`) and `tidy()`
library(lmtest) #bgtest()
library(stargazer) #stargazer()
library(knitr) #kable()
library(sandwich) #coeftest()
library(car) #deltamethod
library(forecast) #forecast()
```
 
 
 
#Question 2
 
Estimate a multiple linear regression model that includes all the main effects only (i.e., no
interactions nor higher order terms). We will use this model as a baseline. Comment on
the statistical and economic significance of your estimates. Also, make sure to provide an
interpretation of your estimates.
 
```{r}
 
suppressMessages(library(stargazer))
model.lm<- lm(default~arm+refinance+lien2+term30+underwater+ltv+rate+amount+fico)
#testing against GLM model due to binomial coef
model.glm<- glm(default~arm+refinance+lien2+term30+underwater+ltv+rate+amount+fico, family = "binomial")
#standard model summaries
summary(model.lm)
summary(model.glm)
#stargazer quick comparison
stargazer(model.lm, model.glm,
          type = "text",
          column.labels = c("LM", "GLM"),
          model.names=FALSE, dep.var.labels.include=FALSE,
          dep.var.caption      =c("models"), digits=5,
          intercept.top = TRUE)
```
Insignificant variables: arm, term30, amount
 
Arm: interesting that arm is not significant since my intuition is that people
who obtain an arm loan with a lower initial payment would cause higher defaults
if the arm causes mortgage payments to increase from its initial value
 
term30: may be insignificant for a few reasons. First, the amount of records
with 15 years is low compared the entire data set. Second, borrowers with 15 year
loans may be more financially fit than 30 year borrowers. Third, the borrow can
refinance to a 30 years to lower payments if needed.
 
amount: I am surprised amount is insignificant as my intuition would be borrowers
with high loan amounts would have a higher likelihood to default
 
# Question 3
 
```{r}
#initial setup
 
t.critical <- function(alpha, df, tail = "two") {
  if(tail == "two"){
    result <- qt(1-alpha/2  , df)
  } else if(tail == "right"){
    result <- qt(1-alpha  , df)
  } else if(tail=="left"){
    result <- qt(alpha  , df)
  }
  return(result)
}
#data
library(POE5Rdata)
data("vegas5")
attach(vegas5)
#Libraries
suppressMessages(library(POE5Rdata))
suppressMessages(library(broom)) #for `glance(`) and `tidy()`
suppressMessages(library(lmtest)) #bgtest()
suppressMessages(library(stargazer)) #stargazer()
suppressMessages(library(knitr)) #kable()
suppressMessages(library(sandwich)) #coeftest()
suppressMessages(library(car)) #deltamethod qqPlot()
suppressMessages(library(forecast)) #forecast()
suppressMessages(library(olsrr)) #ols_plot_resid_lev()
```
 
 
 
```{r}
 
model.lm<- lm(default~arm+refinance+lien2+term30+underwater+ltv+rate+amount+fico)
#visual
# qqPlot(model.lm, id=list(n=3))
#Bonferroni test
outlierTest(model.lm)
 
influenceIndexPlot(model.lm, id=list(n=3),vars="hat")
#ols plots
ols_plot_resid_lev(model.lm)
ols_plot_dfbetas(model.lm)
ols_plot_dffits(model.lm)
 
```
 
# Question 4
 
Based on the output in the table above, lm regression makes more sense, since if we set all the variables to zero, the probability of default equals 0.50, which is equivalent to a flip of a coin. This, intuitively, makes more sense, as it adds more randomness to the event and also because probabilites are positive values in the interval between (0,1). Therefore, we will be using the lm model to answer questions that follow.
 
 
```{r}
suppressMessages(library(leaps))
suppressMessages(library(AER))
suppressMessages(library(broom))
suppressMessages(library(Boruta))
model.lm<- lm(default~arm+refinance+lien2+term30+underwater+ltv+rate+amount+fico,
              data = vegas5)
lm.mallows.cp <- regsubsets(default~arm+refinance+lien2+term30+underwater+ltv+rate+amount+fico,data = vegas5, method = c("exhaustive"), nbest = 8)
plot(lm.mallows.cp)
subsets <- subsets(lm.mallows.cp, statistic = "cp", legend = F, main = "Mallows CP",
                   ylim = c(0,12))
 
 
Boruta.res <- Boruta(default~.,
                     data = vegas5,
                     doTrace = 3,
                     maxRuns = 777)
plot(Boruta.res)
attStats(Boruta.res)[order(-attStats(Boruta.res)$meanImp),]
```
It looks like mallows CP prefers 7 predictor variables since, they are: arm, refinance, lien2, underwater,ltv,rate,fico. In addition, based on Boruta's importance plot, lien2 is rejected as a predictor variable. Hence, we'll trust Boruta that performed 777 max runs and proceed to estimate a model without lien2.
 
# Question 5
 
```{r}
suppressMessages(library(car))
updated.lm <- lm(default~arm+refinance+term30+underwater+ltv+rate+amount+fico,
              data = vegas5)
kable(tidy(summary(updated.lm)))
 
vif(updated.lm)
 
```
 
It seems as though VIF's look fine and none are above 4 which we use as a threshold in this case. Therefore, we will not be removing any variables since there is not a reason to believe that there is strong multicollinearity.
 
# Question 6
 
```{r}
 
ggplot() +
  geom_point(aes(x = updated.lm$fitted.values,
                 y = resid(updated.lm)))
qqPlot(updated.lm)
 
 
```
Again, nothing too suspicious. QQplot confirms our residual plot as well, and our observations almost perfectly follow a logistic curve.
 
# Question 7
 
```{r}
resettest(updated.lm, power = 2, type = "regressor")
 
```
Based on the results, we need to consider including higher-order terms.
 
 
# Question 8
 
```{r}

#accounting for heteroskedasticity 
suppressMessages(library(estimatr))
bptest(updated.lm)
 
updated.lm.robust <- lm_robust(default~arm+refinance+term30+underwater+ltv+rate+log(amount)+fico,
              data = vegas5,
              se_type = "HC1")
bptest(updated.lm.robust)

 
p <- fitted(updated.lm.robust)
p[p<0.01] <- 0.01 #truncating probabilities that don't fall in (0,1) interval
p[p>0.99] <- 0.99
sigma.sq <- p*(1-p)
weight <- 1/sigma.sq
 
updated.lm.robust.trunc <- lm_robust(default~arm+refinance+term30+underwater+ltv+rate+log(amount)+fico, #re-estimating the model
              data = vegas5,
              se_type = "HC1",
              weights = weight)
bptest(updated.lm.robust.trunc)

summary(updated.lm.robust.trunc)


#removing variables

updated.lm.robust <- lm_robust(default~refinance+term30+underwater+ltv+rate+log(amount)+fico,
              data = vegas5,
              se_type = "HC1")
bptest(updated.lm.robust)

 
p <- fitted(updated.lm.robust)
p[p<0.01] <- 0.01 #truncating probabilities that don't fall in (0,1) interval
p[p>0.99] <- 0.99
sigma.sq <- p*(1-p)
weight <- 1/sigma.sq
 
updated.lm.robust.trunc <- lm_robust(default~refinance+underwater+ltv+rate+log(amount)+fico, #re-estimating the model
              data = vegas5,
              se_type = "HC1",
              weights = weight)
bptest(updated.lm.robust.trunc)

resettest(updated.lm.robust.trunc, power = 2, type = "regressor")

summary(updated.lm.robust.trunc)

```
 
Based on the Breusch-Pagan Test, heteroskedasticity is present as expected. After removing the variables that are not statistically significant, we are confident that our estimates for the coefficients are unbiased. By applying Heteroskedasticity-Consistent standard errors and feasible generalized least squares, we have accounted for heteroskedasticity.  And as \(n\rightarrow\infty\) our standard errors, t-tests, interval estimates are valid in large samples. 



# Question 9

```{r}

#re-estimating the model
updated.lm.robust.trunc <- lm_robust(default~refinance+underwater+ltv+I(ltv^2)+rate+amount+fico, 
              data = vegas5,
              se_type = "HC1",
              weights = weight)
summary(updated.lm.robust.trunc)


BIC(model.lm)
BIC(model.glm)


AIC(model.lm)
AIC(model.glm)



```

To retain best interpretability, we will now proceed with lm model where we will also apply FGLS. In terms of AIC and BIC, model.glm has the lowest AIC and BIC.



```{r}

model.lm <- lm(default~arm+refinance+lien2+underwater+ltv+rate+log(amount)+fico,
              data = vegas5)
summary(model.lm)
p <- fitted(model.lm)
p[p<0.01] <- 0.01 #truncating probabilities that don't fall in (0,1) interval
p[p>0.99] <- 0.99
sigma.sq <- p*(1-p)
weight <- 1/sigma.sq

model.lm.fgls <- lm(default~arm+refinance+lien2+underwater+ltv+rate+log(amount)+fico,
              data = vegas5, 
              weights = weight)
summary(model.lm.fgls)

#we will now remove arm to finalize our model, arm is not statistically
# significant

model.lm <- lm(default~refinance+lien2+underwater+ltv+rate+log(amount)+fico,
              data = vegas5)
summary(model.lm)
p <- fitted(model.lm)
p[p<0.01] <- 0.01 #truncating probabilities that don't fall in (0,1) interval
p[p>0.99] <- 0.99
sigma.sq <- p*(1-p)
weight <- 1/sigma.sq

model.lm.fgls <- lm(default~refinance+lien2+underwater+ltv+rate+log(amount)+fico,
              data = vegas5, 
              weights = weight)
summary(model.lm.fgls)

options(digits = 5)
kable(tidy(model.lm.fgls)) #much better



```

Now, since we attained our best lm model, we will apply cross-validation.


```{r}
set.seed(123)
row.number <- sample(1:nrow(vegas5), 0.75*nrow(vegas5))
training_set <- vegas5[row.number,]
test_set <- vegas5[-row.number,]
dim(training_set)


model.lm.1 <- lm(default~refinance+lien2+underwater+ltv+rate+log(amount)+fico,
              data = training_set)
summary(model.lm.1)
p <- fitted(model.lm.1)
p[p<0.01] <- 0.01 #truncating probabilities that don't fall in (0,1) interval
p[p>0.99] <- 0.99
sigma.sq <- p*(1-p)
weight1 <- 1/sigma.sq

model.lm.fgls.1 <- lm(default~refinance+lien2+underwater+ltv+rate+log(amount)+fico,
              data = training_set, 
              weights = weight1)
summary(model.lm.fgls.1)
plot(model.lm.fgls.1)


#calculating RMSE
sqrt(mean((training_set$default-predict(model.lm.fgls.1,training_set))^2)) 
#the predicted value is off by 0.45747

sqrt(mean((test_set$default-predict(model.lm.fgls.1,test_set))^2)) 
#on average the predicted value is off by 0.45011

suppressMessages(library(lmvar))
fit <- lm(default~refinance+lien2+underwater+ltv+rate+log(amount)+fico,
          data = vegas5, 
              weights = weight, 
          x = TRUE, 
          y = TRUE)
          
cv.lm(fit, k = 10)
#RMSE 0.41451

```



